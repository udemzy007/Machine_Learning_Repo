{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b24239",
   "metadata": {},
   "source": [
    "# Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48c134b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd604a9",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d01afff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ~\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:23<?, ? url/s]\n",
      "Dl Size...:   0%|          | 0/1 [00:23<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:23<?, ? url/s]\n",
      "Dl Size...:   0%|          | 0/1 [00:23<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:23<01:10, 23.49s/ url]\n",
      "Dl Size...:   0%|          | 0/1 [00:23<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:23<01:10, 23.49s/ url]\n",
      "Dl Size...:   0%|          | 0/1 [00:23<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:   0%|          | 0/1 [00:23<?, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:23<01:10, 23.49s/ url]4s/ file]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/1 [00:23<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 1/1 [00:23<00:00, 23.74s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:24<01:10, 23.49s/ url]\n",
      "Dl Size...: 100%|██████████| 1/1 [00:24<00:00, 24.44s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:24<00:21, 10.55s/ url]4s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 1/1 [00:24<00:00, 24.44s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:25<00:21, 10.55s/ url]4s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 1/1 [00:25<00:00, 24.44s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...:  50%|█████     | 1/2 [00:25<00:23, 23.74s/ file]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:25<00:21, 10.55s/ url]0s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 1/1 [00:25<00:00, 24.44s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [01:04<00:21, 10.55s/ url]0s/ file]\u001b[A\u001b[A\n",
      "Dl Size...:  10%|█         | 1/10 [01:04<03:39, 24.44s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [01:05<00:21, 10.55s/ url]0s/ file]\u001b[A\u001b[A\n",
      "Dl Size...:  10%|█         | 1/10 [01:05<03:39, 24.44s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:05<00:24, 24.13s/ url]0s/ file]\u001b[A\u001b[A\n",
      "Dl Size...:  10%|█         | 1/10 [01:05<03:39, 24.44s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:05<00:24, 24.13s/ url]0s/ file]\u001b[A\u001b[A\n",
      "Dl Size...:  10%|█         | 1/10 [01:05<03:39, 24.44s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...:  67%|██████▋   | 2/3 [01:05<00:10, 11.00s/ file]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:05<00:24, 24.13s/ url]6s/ file]\u001b[A\u001b[A\n",
      "Dl Size...:  10%|█         | 1/10 [01:05<03:39, 24.44s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [01:05<00:00, 24.06s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:08<00:24, 24.13s/ url]\n",
      "Dl Size...:  20%|██        | 2/10 [01:08<04:49, 36.15s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [01:08<00:00, 24.06s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:11<00:24, 24.13s/ url]\n",
      "Dl Size...:  30%|███       | 3/10 [01:11<02:24, 20.70s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:11<00:24, 24.13s/ url]6s/ file]\u001b[A\u001b[A\n",
      "Dl Size...:  40%|████      | 4/10 [01:11<02:04, 20.70s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [01:11<00:00, 24.06s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:14<00:24, 24.13s/ url]\n",
      "Dl Size...:  50%|█████     | 5/10 [01:14<00:49,  9.98s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [01:14<00:00, 24.06s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:16<00:24, 24.13s/ url]\n",
      "Dl Size...:  60%|██████    | 6/10 [01:16<00:30,  7.60s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [01:16<00:00, 24.06s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:17<00:24, 24.13s/ url]\n",
      "Dl Size...:  70%|███████   | 7/10 [01:17<00:17,  5.73s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:17<00:24, 24.13s/ url]6s/ file]\u001b[A\u001b[A\n",
      "Dl Size...:  80%|████████  | 8/10 [01:17<00:11,  5.73s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [01:17<00:00, 24.06s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:18<00:24, 24.13s/ url]\n",
      "Dl Size...:  90%|█████████ | 9/10 [01:18<00:03,  3.35s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [01:18<00:00, 24.06s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:19<00:24, 24.13s/ url]\n",
      "Dl Size...: 100%|██████████| 10/10 [01:19<00:00,  2.67s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [01:19<00:00, 20.28s/ url]6s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [01:19<00:00,  2.67s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [01:19<00:00, 20.28s/ url]6s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [01:19<00:00,  2.67s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...:  75%|███████▌  | 3/4 [01:19<00:24, 24.06s/ file]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [01:22<00:00, 20.28s/ url]7s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [01:22<00:00,  2.67s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 4/4 [01:22<00:00, 20.74s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [01:22<00:00,  8.30s/ MiB]\n",
      "Dl Completed...: 100%|██████████| 4/4 [01:22<00:00, 20.74s/ url]\n",
      "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]\n",
      "Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "                                                                A\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__(): incompatible constructor arguments. The following argument types are supported:\n    1. tensorflow.python._pywrap_file_io.BufferedInputStream(arg0: str, arg1: int)\n\nInvoked with: WindowsGPath('C:\\\\Users\\\\Udeme\\\\tensorflow_datasets\\\\downloads\\\\extracted\\\\GZIP.cvdf-datasets_mnist_train-images-idx3-ubyteRA_Kv3PMVG-iFHXoHqNwJlYF9WviEKQCTSyo8gNSNgk.gz'), 524288",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# tfds.load(name, with_info, as_supervised) loads the datasets from TensorFlow dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -> as_supervised = True, loads the data in a 2-tuple structure[input, target]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# -> with_info = True, provides a tuple containing info about version, features, # sample of the datasets\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m mnist_dataset, mnist_info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:327\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    326\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 327\u001b[0m   \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:481\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    479\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    487\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    488\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    489\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    490\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1218\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1207\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m split_name, generator \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   1208\u001b[0m       split_generators\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1209\u001b[0m       desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating splits...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1210\u001b[0m       unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m splits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1211\u001b[0m       leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1212\u001b[0m   ):\n\u001b[0;32m   1213\u001b[0m     filename_template \u001b[38;5;241m=\u001b[39m naming\u001b[38;5;241m.\u001b[39mShardedFileTemplate(\n\u001b[0;32m   1214\u001b[0m         split\u001b[38;5;241m=\u001b[39msplit_name,\n\u001b[0;32m   1215\u001b[0m         dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   1216\u001b[0m         data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[0;32m   1217\u001b[0m         filetype_suffix\u001b[38;5;241m=\u001b[39mpath_suffix)\n\u001b[1;32m-> 1218\u001b[0m     future \u001b[38;5;241m=\u001b[39m \u001b[43msplit_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_split_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1224\u001b[0m     split_info_futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;66;03m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:310\u001b[0m, in \u001b[0;36mSplitBuilder.submit_split_generation\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Depending on the type of generator, we use the corresponding\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# `_build_from_xyz` method.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generator, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[1;32m--> 310\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_from_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Otherwise, beam required\u001b[39;00m\n\u001b[0;32m    312\u001b[0m   unknown_generator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid split generator value for split `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    314\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected generator or apache_beam object. Got: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    315\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(generator)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:371\u001b[0m, in \u001b[0;36mSplitBuilder._build_from_generator\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    361\u001b[0m     total_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    363\u001b[0m writer \u001b[38;5;241m=\u001b[39m writer_lib\u001b[38;5;241m.\u001b[39mWriter(\n\u001b[0;32m    364\u001b[0m     example_specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features\u001b[38;5;241m.\u001b[39mget_serialized_info(),\n\u001b[0;32m    365\u001b[0m     filename_template\u001b[38;5;241m=\u001b[39mfilename_template,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m     file_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_format,\n\u001b[0;32m    370\u001b[0m )\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m    372\u001b[0m     generator,\n\u001b[0;32m    373\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples...\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    374\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    375\u001b[0m     total\u001b[38;5;241m=\u001b[39mtotal_num_examples,\n\u001b[0;32m    376\u001b[0m     leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    377\u001b[0m ):\n\u001b[0;32m    378\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    379\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features\u001b[38;5;241m.\u001b[39mencode_example(example)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\image_classification\\mnist.py:151\u001b[0m, in \u001b[0;36mMNIST._generate_examples\u001b[1;34m(self, num_examples, data_path, label_path)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_examples\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_examples, data_path, label_path):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;124;03m\"\"\"Generate MNIST examples as dicts.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m    Generator yielding the next examples\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m   images \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_mnist_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m   labels \u001b[38;5;241m=\u001b[39m _extract_mnist_labels(label_path, num_examples)\n\u001b[0;32m    153\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(images, labels))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\image_classification\\mnist.py:350\u001b[0m, in \u001b[0;36m_extract_mnist_images\u001b[1;34m(image_filepath, num_images)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_mnist_images\u001b[39m(image_filepath, num_images):\n\u001b[0;32m    349\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(image_filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 350\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# header\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     buf \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(_MNIST_IMAGE_SIZE \u001b[38;5;241m*\u001b[39m _MNIST_IMAGE_SIZE \u001b[38;5;241m*\u001b[39m num_images)\n\u001b[0;32m    352\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(\n\u001b[0;32m    353\u001b[0m         buf,\n\u001b[0;32m    354\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8,\n\u001b[0;32m    355\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(num_images, _MNIST_IMAGE_SIZE, _MNIST_IMAGE_SIZE, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:116\u001b[0m, in \u001b[0;36mFileIO.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    105\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns the contents of a file as a string.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m  Starts reading from current position in file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    string if in string (regular) mode.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preread_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    118\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:78\u001b[0m, in \u001b[0;36mFileIO._preread_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_check_passed:\n\u001b[0;32m     76\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mPermissionDeniedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     77\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt open for reading\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_buf \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBufferedInputStream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__(): incompatible constructor arguments. The following argument types are supported:\n    1. tensorflow.python._pywrap_file_io.BufferedInputStream(arg0: str, arg1: int)\n\nInvoked with: WindowsGPath('C:\\\\Users\\\\Udeme\\\\tensorflow_datasets\\\\downloads\\\\extracted\\\\GZIP.cvdf-datasets_mnist_train-images-idx3-ubyteRA_Kv3PMVG-iFHXoHqNwJlYF9WviEKQCTSyo8gNSNgk.gz'), 524288"
     ]
    }
   ],
   "source": [
    "# tfds.load(name, with_info, as_supervised) loads the datasets from TensorFlow dataset\n",
    "# -> as_supervised = True, loads the data in a 2-tuple structure[input, target]\n",
    "# -> with_info = True, provides a tuple containing info about version, features, # sample of the datasets\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info = True, as_supervised= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56dc73e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ~\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 55.59 url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 38.73 url/s]\n",
      "Dl Size...:   0%|          | 0/1648877 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 30.50 url/s]\n",
      "Dl Size...: 100%|██████████| 1648877/1648877 [00:00<00:00, 53567129.56 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 1/2 [00:00<00:00, 23.38 url/s]\n",
      "Dl Size...: 100%|██████████| 1648877/1648877 [00:00<00:00, 39467733.06 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 37.90 url/s]\n",
      "Dl Size...: 100%|██████████| 1648877/1648877 [00:00<00:00, 30666557.57 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 32.38 url/s]\n",
      "Dl Size...: 100%|█████████▉| 1648877/1653419 [00:00<00:00, 26696408.11 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 27.49 url/s]\n",
      "Dl Size...: 100%|██████████| 1653419/1653419 [00:00<00:00, 22725071.60 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  67%|██████▋   | 2/3 [00:00<00:00, 23.60 url/s]\n",
      "Dl Size...: 100%|██████████| 1653419/1653419 [00:00<00:00, 19509273.45 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 32.35 url/s]\n",
      "Dl Size...: 100%|██████████| 1653419/1653419 [00:00<00:00, 17827247.58 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 29.20 url/s]\n",
      "Dl Size...:  14%|█▍        | 1653419/11565841 [00:00<00:00, 16093078.24 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 25.27 url/s]593356.73 MiB/s]\u001b[A\n",
      "Dl Size...: 100%|██████████| 11565841/11565841 [00:00<00:00, 102593356.73 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00, 19.27 url/s]\n",
      "Dl Size...: 100%|██████████| 11565841/11565841 [00:00<00:00, 102593356.73 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 24.29 url/s]\n",
      "Dl Size...: 100%|██████████| 11565841/11565841 [00:00<00:00, 102593356.73 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 24.29 url/s]\n",
      "Dl Size...: 100%|█████████▉| 11565841/11594722 [00:00<00:00, 102593356.73 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 24.29 url/s]\n",
      "Dl Size...: 100%|██████████| 11594722/11594722 [00:00<00:00, 102593356.73 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 11594722/11594722 [00:00<00:00, 50049284.70 MiB/s] \n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 16.02 url/s]\n",
      "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]\n",
      "Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "                                                                A\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__(): incompatible constructor arguments. The following argument types are supported:\n    1. tensorflow.python._pywrap_file_io.BufferedInputStream(arg0: str, arg1: int)\n\nInvoked with: WindowsGPath('C:\\\\Users\\\\Udeme\\\\tensorflow_datasets\\\\downloads\\\\extracted\\\\GZIP.cvdf-datasets_mnist_train-images-idx3-ubyteRA_Kv3PMVG-iFHXoHqNwJlYF9WviEKQCTSyo8gNSNgk.gz'), 524288",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mnist_dataset, mnist_info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:327\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    326\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 327\u001b[0m   \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:481\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    479\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    487\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    488\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    489\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    490\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1218\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1207\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m split_name, generator \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   1208\u001b[0m       split_generators\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1209\u001b[0m       desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating splits...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1210\u001b[0m       unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m splits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1211\u001b[0m       leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1212\u001b[0m   ):\n\u001b[0;32m   1213\u001b[0m     filename_template \u001b[38;5;241m=\u001b[39m naming\u001b[38;5;241m.\u001b[39mShardedFileTemplate(\n\u001b[0;32m   1214\u001b[0m         split\u001b[38;5;241m=\u001b[39msplit_name,\n\u001b[0;32m   1215\u001b[0m         dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   1216\u001b[0m         data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[0;32m   1217\u001b[0m         filetype_suffix\u001b[38;5;241m=\u001b[39mpath_suffix)\n\u001b[1;32m-> 1218\u001b[0m     future \u001b[38;5;241m=\u001b[39m \u001b[43msplit_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_split_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1224\u001b[0m     split_info_futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;66;03m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:310\u001b[0m, in \u001b[0;36mSplitBuilder.submit_split_generation\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Depending on the type of generator, we use the corresponding\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# `_build_from_xyz` method.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generator, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[1;32m--> 310\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_from_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Otherwise, beam required\u001b[39;00m\n\u001b[0;32m    312\u001b[0m   unknown_generator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid split generator value for split `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    314\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected generator or apache_beam object. Got: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    315\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(generator)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:371\u001b[0m, in \u001b[0;36mSplitBuilder._build_from_generator\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    361\u001b[0m     total_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    363\u001b[0m writer \u001b[38;5;241m=\u001b[39m writer_lib\u001b[38;5;241m.\u001b[39mWriter(\n\u001b[0;32m    364\u001b[0m     example_specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features\u001b[38;5;241m.\u001b[39mget_serialized_info(),\n\u001b[0;32m    365\u001b[0m     filename_template\u001b[38;5;241m=\u001b[39mfilename_template,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m     file_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_format,\n\u001b[0;32m    370\u001b[0m )\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m    372\u001b[0m     generator,\n\u001b[0;32m    373\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples...\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    374\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    375\u001b[0m     total\u001b[38;5;241m=\u001b[39mtotal_num_examples,\n\u001b[0;32m    376\u001b[0m     leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    377\u001b[0m ):\n\u001b[0;32m    378\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    379\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features\u001b[38;5;241m.\u001b[39mencode_example(example)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\image_classification\\mnist.py:151\u001b[0m, in \u001b[0;36mMNIST._generate_examples\u001b[1;34m(self, num_examples, data_path, label_path)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_examples\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_examples, data_path, label_path):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;124;03m\"\"\"Generate MNIST examples as dicts.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m    Generator yielding the next examples\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m   images \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_mnist_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m   labels \u001b[38;5;241m=\u001b[39m _extract_mnist_labels(label_path, num_examples)\n\u001b[0;32m    153\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(images, labels))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow_datasets\\image_classification\\mnist.py:350\u001b[0m, in \u001b[0;36m_extract_mnist_images\u001b[1;34m(image_filepath, num_images)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_mnist_images\u001b[39m(image_filepath, num_images):\n\u001b[0;32m    349\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(image_filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 350\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# header\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     buf \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(_MNIST_IMAGE_SIZE \u001b[38;5;241m*\u001b[39m _MNIST_IMAGE_SIZE \u001b[38;5;241m*\u001b[39m num_images)\n\u001b[0;32m    352\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(\n\u001b[0;32m    353\u001b[0m         buf,\n\u001b[0;32m    354\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8,\n\u001b[0;32m    355\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(num_images, _MNIST_IMAGE_SIZE, _MNIST_IMAGE_SIZE, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:116\u001b[0m, in \u001b[0;36mFileIO.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    105\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns the contents of a file as a string.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m  Starts reading from current position in file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    string if in string (regular) mode.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preread_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    118\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iris_2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:78\u001b[0m, in \u001b[0;36mFileIO._preread_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_check_passed:\n\u001b[0;32m     76\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mPermissionDeniedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     77\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt open for reading\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_buf \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBufferedInputStream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__(): incompatible constructor arguments. The following argument types are supported:\n    1. tensorflow.python._pywrap_file_io.BufferedInputStream(arg0: str, arg1: int)\n\nInvoked with: WindowsGPath('C:\\\\Users\\\\Udeme\\\\tensorflow_datasets\\\\downloads\\\\extracted\\\\GZIP.cvdf-datasets_mnist_train-images-idx3-ubyteRA_Kv3PMVG-iFHXoHqNwJlYF9WviEKQCTSyo8gNSNgk.gz'), 524288"
     ]
    }
   ],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info = True, as_supervised= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376bd041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c6b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
